{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"pidTraining_LE.hdf5\"\n",
    "filename = \"/home/ech18004/PID/data/\" + file\n",
    "\n",
    "train = pd.read_hdf(filename, 'event1')\n",
    "\n",
    "file = \"pidTest_LE.hdf5\"\n",
    "filename = \"/home/ech18004/PID/data/\" + file\n",
    "\n",
    "test = pd.read_hdf(filename, 'event1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the labels of the data (the training and test data have the same labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train.columns)):\n",
    "    print(i, train.columns[i]) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See what particle types are avialable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptypes = np.array([np.int64(train['ptype'][i]) for i in range(0, len(train), 80000)])\n",
    "ptype = {22:0, 130:1, 2112:2, 2212:3, -2212:4, 321:5, -321:6, 11:7, -11:8, 211:9, -211:10}\n",
    "val_to_p = {22:r'$\\gamma$', 130:r'$K_{L}^{0}$', 2112:r'$n$', 2212:r'$p$', -2212:r'$\\bar{p}$', 321:r'$K^{+}$', -321:r'$K^{-}$', 11:r'$e^{-}$', -11:r'$e^{+}$', 211:r'$\\pi^{+}$', -211:r'$\\pi^{-}$'}\n",
    "ptypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unwanted labels from test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testnp = pd.DataFrame.to_numpy(test)\n",
    "testind = np.array([i for i in range(len(testnp)) if np.float(testnp[i][0]) == testnp[i][-1]])\n",
    "testout = test.loc[testind]\n",
    "testout = test.drop(['ptype', 'group'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate data into numpy arrays and set nans to a float value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repval = 0\n",
    "listvals = np.array([27, 28, 31, 32])\n",
    "listvals = ['posShower', 'posTrack', 'posTOF', 'tFlights']\n",
    "\n",
    "train = train.replace(np.nan, repval)\n",
    "testout = testout.replace(np.nan, repval)\n",
    "\n",
    "train = train.drop(listvals, axis=1)\n",
    "testout = testout.drop(listvals, axis=1)\n",
    "\n",
    "trainx, trainy = pd.DataFrame.to_numpy(train.drop('ptype', axis=1)), pd.DataFrame.to_numpy(train['ptype'])\n",
    "trainy = trainy.astype(np.int64)\n",
    "testx, testy = pd.DataFrame.to_numpy(testout.drop('true ptype', axis=1)), pd.DataFrame.to_numpy(test['true ptype'])\n",
    "\n",
    "trainy = np.array([ptype[trainy[i]] for i in range(len(trainy))])\n",
    "testy = np.array([ptype[testy[i]] for i in range(len(testy))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to tensorflow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train = tf.data.Dataset.from_tensor_slices((trainx, trainy)).cache()\n",
    "tf_test = tf.data.Dataset.from_tensor_slices((testx, testy)).cache()\n",
    "\n",
    "tf_train = tf_train.shuffle(len(tf_train))\n",
    "\n",
    "tf_train = tf_train.batch(128)\n",
    "tf_test = tf_test.batch(128)\n",
    "\n",
    "tf_train = tf_train.prefetch(tf.data.AUTOTUNE)\n",
    "tf_test = tf_test.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Train a DNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "#model.add(tf.keras.layers.Flatten(input_shape=(253,))) ###Only need to flatten input if it was not already previously flattened\n",
    "model.add(tf.keras.layers.Dense(253, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(len(ptypes), activation = 'sigmoid'))\n",
    "\n",
    "#model.compile(optimizer='adam',\n",
    "#              loss='sparse_categorical_crossentropy',\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(), \n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "              metrics = [tf.keras.metrics.SparseCategoricalAccuracy()],)\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.01, patience=5) #, start_from_epoch=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tf_train, \n",
    "          epochs=50, \n",
    "          validation_data=tf_test,\n",
    "          callbacks=[callback],\n",
    "          verbose = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use model to predict on the test data and find the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(testx)\n",
    "predarr = np.array([np.argmax(pred[i]) for i in range(len(pred))])\n",
    "acc = accuracy_score(testy, predarr)\n",
    "acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creat a confusion matrix for the model created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(testy, predarr, normalize='true')\n",
    "#cm = np.array([cm[i]/len(testy[testy == i]) for i in range(len(cm))])\n",
    "cm = np.round(cm, decimals=3)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(10, 10)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax);\n",
    "\n",
    "\n",
    "ax.set_xlabel('Predicted Particle', fontsize=15);ax.set_ylabel('Generated Particle', fontsize=15); \n",
    "ax.set_title(f'PID Confusion Matrix with a Total Accuracy of {round(acc, 4)}', fontsize=17); \n",
    "ax.xaxis.set_ticklabels([r'$\\gamma$', r'$K_{L}^{0}$', r'$n$', r'$p$', r'$\\bar{p}$', \n",
    "                         r'$K^{+}$', r'$K^{-}$', r'$e^{-}$', r'$e^{+}$', r'$\\pi^{+}$', r'$\\pi^{-}$'], fontsize=15);\n",
    "ax.yaxis.set_ticklabels([r'$\\gamma$', r'$K_{L}^{0}$', r'$n$', r'$p$', r'$\\bar{p}$', \n",
    "                         r'$K^{+}$', r'$K^{-}$', r'$e^{-}$', r'$e^{+}$', r'$\\pi^{+}$', r'$\\pi^{-}$'], fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See how well the model worked on data that had no hypotheses that matched the true ptype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to find the indices of each test data with correct particle hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testnp = pd.DataFrame.to_numpy(test)\n",
    "iter = 0\n",
    "no_id = np.array([])\n",
    "for i in range(int(len(train)/2)):        #Divided by 2 since the training dataset is 2x longer than the test dataset\n",
    "    group_list = np.where(test['group'] == i)[0]\n",
    "    count = 0\n",
    "    for j in range(len(group_list)):\n",
    "        if np.float(testnp[iter][0]) == testnp[iter][-1]:\n",
    "            count += 1\n",
    "        iter += 1\n",
    "    if count == 0:\n",
    "        no_id = np.append(no_id, group_list)\n",
    "\n",
    "no_id = no_id.astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcualte accuracy for the vecotrs with incorrect hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy_noID = np.array([testy[i] for i in no_id])\n",
    "predarr_noID = np.array([predarr[i] for i in no_id])\n",
    "\n",
    "acc = accuracy_score(testy_noID, predarr_noID)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add 1 label for each ptype (so confusion_matrix works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    testy_noID = np.append(testy_noID, testy[testy == i][0])\n",
    "    predarr_noID = np.append(predarr_noID, predarr[predarr == i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print a confusion matrix for the vectors with incorrect hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(testy_noID, predarr_noID, normalize='true')\n",
    "#cm = np.array([cm[i]/len(testy_noID[testy_noID == i]) for i in range(len(cm))])\n",
    "cm = np.round(cm, decimals=3)\n",
    "\n",
    "for i in range(3):         ##Set the extra values to zero\n",
    "    cm[i][i] = 0\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(10, 10)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax);\n",
    "\n",
    "\n",
    "ax.set_xlabel('Predicted Particle', fontsize=15);ax.set_ylabel('Generated Particle', fontsize=15); \n",
    "ax.set_title(f'PID Confusion Matrix with a Total Accuracy of {round(acc, 4)}', fontsize=17); \n",
    "ax.xaxis.set_ticklabels([r'$\\gamma$', r'$K_{L}^{0}$', r'$n$', r'$p$', r'$\\bar{p}$', \n",
    "                         r'$K^{+}$', r'$K^{-}$', r'$e^{-}$', r'$e^{+}$', r'$\\pi^{+}$', r'$\\pi^{-}$'], fontsize=15);\n",
    "ax.yaxis.set_ticklabels([r'$\\gamma$', r'$K_{L}^{0}$', r'$n$', r'$p$', r'$\\bar{p}$', \n",
    "                         r'$K^{+}$', r'$K^{-}$', r'$e^{-}$', r'$e^{+}$', r'$\\pi^{+}$', r'$\\pi^{-}$'], fontsize=15);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
